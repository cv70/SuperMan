# SuperMan：基于强化学习的端到端自主智能体

## 概述

SuperMan 是一种创新框架，通过监督微调（SFT）与强化学习（RL）相结合，将智能体决策能力直接嵌入大语言模型（LLM）中，彻底摆脱对僵化预定义工作流的依赖。与传统基于工作流的智能体（如 ReAct、Coze、Dify）依赖显式步骤提示或拖拽式流程不同，SuperMan 首先通过 SFT 使模型掌握基础任务执行能力，再通过 RL 在开放性环境中自主探索、推理与行动，学习最优行动策略。

## 传统工作流智能体的缺陷

传统智能体框架存在以下关键局限：

- **僵化的步骤分解**：开放性任务（如“从这份数据中挖掘有价值的洞察”）必须人工拆解为固定步骤。若预定义流程未覆盖某种探索路径，智能体将失效或陷入死循环。
- **提示膨胀**：ReAct 等系统通过复杂系统提示强制模型遵循“思考-行动-观察”固定循环，提示工程日益臃肿，维护和调试变得脆弱且不可扩展。
- **上下文过载**：错误信息与环境反馈作为纯文本追加至对话历史，再次输入模型，过度依赖大模型的通用上下文理解能力，而非内化决策能力。
- **缺乏真正自主性**：智能体并非在学习“如何探索”，而是在被“告诉”如何探索。

## SuperMan：基于强化学习的自主决策

SuperMan 重新定义了智能体架构，将行为建模为通过强化学习学习的**概率性策略**。

### 核心机制

- **状态（State）**：当前环境上下文（数据结构、历史操作、观察结果、错误信息）。
- **动作空间（Action Space）**：高度通用的原子操作集合，涵盖数据理解、工具调用、推理控制与环境交互，包括但不限于，可以根据自己的场景添加或删除动作：
  - `Analyze`（分析）：统计摘要、分布检验、异常检测
  - `Code`（编码）：生成/执行 Python/SQL/R 代码，调用库函数
  - `Query`（查询）：检索外部知识库、数据库、API
  - `Visualize`（可视化）：生成图表、交互式仪表盘
  - `Understand`（理解）：总结文本、解释模型输出、识别语义意图
  - `Refine`（优化）：修正错误、迭代假设、调整参数
  - `Explore`（探索）：主动尝试未知路径，生成新假设
  - `Plan`（规划）：分解复杂任务，构建多步执行策略
  - `Reflect`（反思）：评估当前进展，回溯失败原因
  - `Stop`（终止）：结束当前任务并输出最终结果
  - `Ask`（询问）：向用户请求澄清或反馈（可选人机协作）
  - `Store`（存储）：缓存中间结果、知识或模式以供复用
  - `Compare`（比较）：对比多个假设、模型或结果
  - `Simulate`（模拟）：预测行动后果，进行虚拟推演
  - `Summarize`（总结）：提炼核心结论，构建高层抽象
  - `Validate`（验证）：通过交叉检验、基准测试确认正确性
  - `Adapt`（自适应）：根据环境变化动态调整策略
  - `Learn`（学习）：从新经验中更新内部表征（在线微调）
- **奖励信号（Reward Signal）**：基于任务完成质量、执行效率、探索深度、结果创新性与可解释性综合计算，支持多目标优化。
- **策略（Policy）**：一个经过监督微调（SFT）与强化学习（RL）联合训练的大语言模型，将状态映射为动作概率。SFT 提供初始行为基线，RL 优化长期回报。

模型不再被提示遵循脚本，而是学习：  
> _“根据当前数据和历史，采取哪种行动能最大化长期累积回报？”_

### 端到端自主性

- 无需外部工作流引擎。
- 无需人工编写的步骤序列。
- 无需为控制流进行提示工程。
- 模型自主决策：  
  → 我该检查列分布吗？  
  → 我该生成相关性热力图吗？  
  → 我该写 SQL 查询还是 Python 脚本？  
  → 我该回溯并尝试其他假设吗？

该决策非模板化——它是**涌现性**的，通过成千上万多样化任务的奖励信号学习而来。

## 核心优势

| 特性 | 传统智能体 | SuperMan |
|------|------------|----------|
| 决策逻辑 | 硬编码于提示/工作流 | 通过 RL 学习，内嵌于模型权重 |
| 适应性 | 仅限预定义路径 | 可泛化至未见过的任务与数据 |
| 可扩展性 | 提示复杂度指数增长 | 策略随模型容量扩展 |
| 调试方式 | 通过工作流日志追踪步骤 | 分析策略 logits 与奖励梯度 |
| 维护方式 | 手动提示调优 | 模型微调 + 奖励设计 |
| 探索能力 | 仅限已知模式 | 发现新颖、非显而易见的洞察 |

## 使用方式

1. **训练**：使用强化学习，在精心构建的开放性数据分析任务数据集上微调基础 LLM（如 Qwen、Deepseek、Llama 等）。
2. **部署**：加载训练好的模型作为独立智能体，无需任何外部编排。
3. **交互**：提供原始数据与高层目标，智能体将自主探索。

```python
from superman import SuperManAgent

agent = SuperManAgent(model_path="path/to/superman-8b")
result = agent.execute("分析这份 CSV 并找出意外的相关性")
# 智能体自主执行：加载数据 → 检查类型 → 绘制分布图 → 计算相关性 → 生成报告
```

## 架构

```mermaid
graph TD
    A[原始数据 + 目标] --> B[状态编码器]
    B --> C[策略网络<br>（带 RL 头的大语言模型）]
    C --> D[动作<br>{Analyze, Code, Query, Visualize, Understand, Refine, Explore, Plan, Reflect, Stop, Ask, Store, Compare, Simulate, Summarize, Validate, Adapt, Learn}]
    D --> E[环境<br>（执行动作，返回观察、奖励、工具响应、错误信息）]
    E --> F[策略更新<br>（强化学习算法：PPO/DPO）]
    F --> C
    style A fill:#f9f,stroke:#333
    style B fill:#bbf,stroke:#333
    style C fill:#ff9,stroke:#333
    style D fill:#f96,stroke:#333
    style E fill:#6f9,stroke:#333
    style F fill:#9f9,stroke:#333
```

## 训练流程

1. **数据集**：精心构建的开放性数据分析任务集合（如 Kaggle 数据集、真实商业查询、公开数据源、大模型蒸馏精炼数据等）。
2. **仿真环境**：Python 沙箱，执行智能体动作（数据加载、代码执行、可视化）。
3. **奖励模型**：基于规则或学习的评估器，量化洞察质量、正确性与效率。
4. **监督微调（SFT）**：使用标注的专家示范数据，对基础 LLM 进行监督微调，使其掌握基本任务执行能力。
5. **RL 微调**：使用 PPO 或 DPO 在仿真环境中进一步优化模型策略，最大化长期累积回报。

## 为何重要？

SuperMan 实现了从**提示工程**到**模型训练**的范式转变。  
它将大语言模型从顺从的脚本执行者，转变为**真正的自主问题解决者**——具备真正的求知欲与自适应推理能力。

这不仅是对智能体的改进。  
这是通向**通用人工智能代理**的第一步——无需被告知下一步该做什么。

## 📝 贡献指南

欢迎提交 Issue 和 Pull Request 来改进本项目！

## 📄 许可证

本项目采用 Apache-2.0 许可证，详情请见 [LICENSE](./LICENSE-APACHE) 文件。
